# STEP 3: Engagement Logic

## Definition of Engagement

In the X algorithm, **engagement** is defined as **any user interaction with a post**, tracked across **19 distinct action types**.

---

## The 19 Engagement Actions

### Positive Engagement Actions (increase post score)

1. `favorite_score` - Like/heart the post â¤ï¸
2. `reply_score` - Reply to the post ğŸ’¬
3. `repost_score` - Retweet/share the post ğŸ”„
4. `quote_score` - Quote tweet ğŸ’­
5. `click_score` - Click on the post ğŸ‘†
6. `profile_click_score` - Click on author's profile ğŸ‘¤
7. `photo_expand_score` - Expand images ğŸ–¼ï¸
8. `vqv_score` - Video quality view (watch video) ğŸ“¹
9. `share_score` - Share the post ğŸ“¤
10. `share_via_dm_score` - Share via DM ğŸ’Œ
11. `share_via_copy_link_score` - Copy link ğŸ”—
12. `dwell_score` - Dwell on post (time spent) â±ï¸
13. `quoted_click_score` - Click on quoted tweet ğŸ”
14. `follow_author_score` - Follow the author â•
15. `dwell_time` - Actual time spent (continuous value) â²ï¸

### Negative Engagement Actions (decrease post score)

16. `not_interested_score` - Mark "not interested" ğŸ‘
17. `block_author_score` - Block the author ğŸš«
18. `mute_author_score` - Mute the author ğŸ”‡
19. `report_score` - Report the post âš ï¸

---

## How Engagement is Created

### 1. Real-Time Event Capture (Thunder Service)

```
User Action â†’ Kafka Event Stream â†’ Thunder In-Memory Store
```

**Thunder's Role:**
- Consumes post create/delete events from Kafka
- Maintains per-user stores for recent posts
- Tracks engagement events as they happen
- Provides sub-millisecond lookups for in-network content

### 2. Engagement History Storage

```python
history_actions: [B, S, 19]  # Multi-hot vector per history item
```

**Example engagement vector:**
```python
# User liked and clicked a post, but didn't reply
history_actions[0, 5, :] = [
    1.0,  # favorite_score âœ“
    0.0,  # reply_score
    0.0,  # repost_score
    0.0,  # photo_expand_score
    1.0,  # click_score âœ“
    0.0,  # profile_click_score
    0.0,  # vqv_score
    0.0,  # share_score
    0.0,  # share_via_dm_score
    0.0,  # share_via_copy_link_score
    0.0,  # dwell_score
    0.0,  # quote_score
    0.0,  # quoted_click_score
    0.0,  # follow_author_score
    0.0,  # not_interested_score
    0.0,  # block_author_score
    0.0,  # mute_author_score
    0.0,  # report_score
    0.0,  # dwell_time
]
```

### 3. Engagement Encoding

```python
def _get_action_embeddings(actions):
    # Convert {0,1} multi-hot to {-1,+1} signed vector
    actions_signed = (2 * actions - 1)
    
    # Project to embedding space
    action_emb = dot(actions_signed, action_projection_matrix)
    
    return action_emb
```

**Critical Insight:** Negative actions get **negative embeddings** (`-1`), creating repulsion in the embedding space.

---

## How Engagement is Measured

### 1. Prediction Stage

The transformer outputs **probabilities for all 19 actions**:

```python
logits = transformer(user + history + candidates)  # [B, C, 19]
probs = sigmoid(logits)  # Convert to probabilities [0, 1]
```

### 2. Primary Ranking Metric

```python
# Posts are ranked by FAVORITE_SCORE (index 0)
primary_scores = probs[:, :, 0]
ranked_indices = argsort(-primary_scores)
```

**Why favorite_score?**
- Most common positive engagement
- Strong signal of content quality
- Correlates with other positive engagements

### 3. Weighted Combination (Home Mixer)

```
Final Score = Î£ (weight_i Ã— P(action_i))

Where:
  weight_favorite > 0      (e.g., +2.0)
  weight_reply > 0         (e.g., +1.5)
  weight_repost > 0        (e.g., +1.0)
  weight_block < 0         (e.g., -3.0)
  weight_mute < 0          (e.g., -2.0)
  weight_report < 0        (e.g., -5.0)
```

---

## How Engagement is Updated

### 1. Continuous Learning Loop

```
User sees post â†’ User engages â†’ Event logged â†’ Model retraining â†’ Updated predictions
```

### 2. Embedding Table Updates

```python
# Hash-based embeddings are updated during training
user_embeddings[user_hash] â† gradient_update
post_embeddings[post_hash] â† gradient_update
author_embeddings[author_hash] â† gradient_update
```

### 3. History Sequence Updates

```python
# As user engages, history grows
history_post_hashes = [post_1, post_2, ..., post_32]  # Last 32 interactions
history_actions = [actions_1, actions_2, ..., actions_32]
```

**Retention:**
- Thunder trims posts older than retention period
- History sequence has max length (32 in demo, 128 max)
- Older interactions are dropped (FIFO)

---

## Feedback Loops

### 1. Positive Feedback Loop (Engagement Amplification)

```
User likes post A
  â†“
Model learns: User â†’ Post A embedding similarity â†‘
  â†“
Similar posts (B, C, D) get higher scores
  â†“
User sees more similar content
  â†“
User engages more with similar content
  â†“
Similarity strengthens further
```

**Code Implementation:**
```python
# History embedding includes past actions
history_embedding = project(concat([
    post_embeddings,
    author_embeddings,
    action_embeddings,  # â† Past engagement creates bias
    product_surface_embeddings
]))

# Transformer learns: if user liked similar posts â†’ boost candidate
candidate_score = transformer(user + history + candidate)
```

### 2. Negative Feedback Loop (Engagement Suppression)

```
User blocks author X
  â†“
action_embedding = -1 Ã— action_projection[block_author_score]
  â†“
Negative signal flows through transformer
  â†“
Posts from similar authors get lower scores
  â†“
User sees less content from similar authors
```

**Code Implementation:**
```python
# Signed action embeddings create repulsion
actions_signed = (2 * actions - 1)  # {0,1} â†’ {-1,+1}
action_emb = dot(actions_signed, action_projection)

# Block action (index 15) creates negative embedding
# This suppresses similar candidates in transformer attention
```

### 3. Diversity Feedback Loop (Author Diversity Scorer)

```
Author appears in feed
  â†“
Author Diversity Scorer attenuates repeated author scores
  â†“
Same author's next post gets lower score
  â†“
Feed shows different authors
```

**Purpose:** Prevent feed from being dominated by single author.

### 4. Recency Feedback Loop (Thunder Retention)

```
Post created â†’ Thunder stores â†’ Time passes â†’ Post ages â†’ Thunder trims
```

**Effect:** Old posts naturally drop out of candidate pool, ensuring freshness.

---

## How Past Engagement Affects Future Outcomes

### Mechanism 1: Embedding Space Clustering

```python
# User who liked posts [A, B, C] gets user_embedding positioned near them
user_representation = mean_pool(transformer(user + history))

# Candidates similar in embedding space get higher scores
similarity = dot(user_representation, candidate_embedding)
```

### Mechanism 2: Attention Patterns

```python
# Transformer learns attention patterns like:
# "If user liked tech posts + followed authors â†’ attend to tech candidates"

attention_weights = softmax(
    dot(query, key) / sqrt(d_k)
)

# History with tech engagement â†’ high attention to tech candidates
```

### Mechanism 3: Action-Specific Predictions

```python
# Model learns conditional probabilities:
# P(like | user liked similar posts) > P(like | user ignored similar posts)

# Training objective (implicit):
# Maximize: log P(observed_actions | user_history, candidate)
```

---

## Engagement Feedback Loop Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ENGAGEMENT FEEDBACK LOOP                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    User Sees Feed
         â”‚
         â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ User Engages    â”‚ â† Positive: like, reply, share
    â”‚ with Post       â”‚ â† Negative: block, mute, report
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Event Logged    â”‚
    â”‚ (Kafka Stream)  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Thunder Updates â”‚ â† In-memory post store
    â”‚ History Store   â”‚ â† User action sequence
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Model Training  â”‚ â† Embeddings updated
    â”‚ (Offline)       â”‚ â† Transformer weights updated
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Updated Model   â”‚
    â”‚ Deployed        â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Next Feed       â”‚ â† Predictions reflect past engagement
    â”‚ Request         â”‚ â† Similar content boosted/suppressed
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                            â”‚
                            â–¼
                    User Sees Feed (loop continues)
```

---

## Engagement Persistence

### Short-Term (Seconds to Minutes)
- Thunder in-memory store
- Recent posts from followed accounts
- Real-time candidate availability

### Medium-Term (Hours to Days)
- User action sequence (last 32-128 interactions)
- Embedding table lookups
- Model predictions based on recent history

### Long-Term (Weeks to Months)
- Trained embedding tables
- Transformer weights
- Learned user preferences encoded in parameters

---

## Key Takeaways

1. **Engagement is a closed feedback loop** where past actions influence future content
2. **Signed embeddings** create attraction (positive actions) and repulsion (negative actions)
3. **Transformer learns patterns** from engagement history to predict future engagement
4. **Similar content gets amplified or suppressed** based on past behavior
5. **The algorithm wants users to engage more**, so it continuously learns what drives engagement

**The Core Truth:** Every engagement you make teaches the algorithm what to show you next. Like creates more similar content. Block creates less similar content. The system is always learning and adapting.
